{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fast-image-colorization-title"
   },
   "source": [
    "# Fast Image Colorization with Pix2Pix GAN\n",
    "**Optimized for Google Colab T4 GPU - Complete training in 2-3 hours**\n",
    "\n",
    "This notebook implements a lightweight Pix2Pix GAN for fast image colorization using:\n",
    "- 5,000 image subset from Kaggle dataset\n",
    "- Lightweight U-Net generator (5 blocks, 32 base filters)\n",
    "- PatchGAN discriminator (4 layers)\n",
    "- Mixed precision training\n",
    "- Speed optimizations for Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## üöÄ Automatic Setup for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "‚ö†Ô∏è No GPU detected. This will be very slow on CPU!\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. This will be very slow on CPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q kaggle scikit-image tqdm matplotlib seaborn\n",
    "\n",
    "# Import all necessary libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True  # Enable for speed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaggle-setup"
   },
   "source": [
    "## üìÅ Automatic Kaggle Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kaggle-auth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Setting up Kaggle authentication...\n",
      "Please upload your kaggle.json file when prompted, or create it manually:\n",
      "1. Go to https://www.kaggle.com/account\n",
      "2. Click 'Create New Token'\n",
      "3. Upload the downloaded kaggle.json file\n",
      "‚ÑπÔ∏è Not running in Colab. Please ensure kaggle.json is in ~/.kaggle/\n"
     ]
    }
   ],
   "source": [
    "# Kaggle API setup\n",
    "print(\"üîë Setting up Kaggle authentication...\")\n",
    "print(\"Please upload your kaggle.json file when prompted, or create it manually:\")\n",
    "print(\"1. Go to https://www.kaggle.com/account\")\n",
    "print(\"2. Click 'Create New Token'\")\n",
    "print(\"3. Upload the downloaded kaggle.json file\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Setup Kaggle credentials\n",
    "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "    shutil.move('kaggle.json', '/root/.kaggle/kaggle.json')\n",
    "    os.chmod('/root/.kaggle/kaggle.json', 600)\n",
    "    print(\"‚úÖ Kaggle authentication successful!\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Not running in Colab. Please ensure kaggle.json is in ~/.kaggle/\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Manual setup: Please ensure kaggle.json is properly configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading dataset: shravankumar9892/image-colorization\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/bin/kaggle\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/kaggle/cli.py\", line 68, in main\n",
      "    out = args.func(**command_args)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
      "    with self.build_kaggle_client() as kaggle:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
      "    username=self.config_values['username'],\n",
      "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'username'\n",
      "üì¶ Extracting dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/image-colorization.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mkaggle datasets download -d \u001b[39m\u001b[38;5;132;01m{dataset_name}\u001b[39;00m\u001b[33m -p /content/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müì¶ Extracting dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/content/image-colorization.zip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[32m     11\u001b[39m     zip_ref.extractall(\u001b[33m'\u001b[39m\u001b[33m/content/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Find the extracted folder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/zipfile.py:1284\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         \u001b[38;5;28mself\u001b[39m.fp = io.open(file, filemode)\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1286\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/image-colorization.zip'"
     ]
    }
   ],
   "source": [
    "# Download and extract dataset\n",
    "dataset_name = \"shravankumar9892/image-colorization\"\n",
    "data_dir = Path(\"/content/colorization_data\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(f\"üì• Downloading dataset: {dataset_name}\")\n",
    "    !kaggle datasets download -d {dataset_name} -p /content/\n",
    "    \n",
    "    print(\"üì¶ Extracting dataset...\")\n",
    "    with zipfile.ZipFile('/content/image-colorization.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    \n",
    "    # Find the extracted folder\n",
    "    extracted_folders = [f for f in os.listdir('/content/') if 'colorization' in f.lower() or 'image' in f.lower()]\n",
    "    if extracted_folders:\n",
    "        os.rename(f'/content/{extracted_folders[0]}', data_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset ready at: {data_dir}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset already exists at: {data_dir}\")\n",
    "\n",
    "# List dataset contents\n",
    "if data_dir.exists():\n",
    "    subfolders = [f for f in data_dir.iterdir() if f.is_dir()]\n",
    "    print(f\"\\nüìÇ Dataset structure:\")\n",
    "    for folder in subfolders:\n",
    "        img_count = len(list(folder.glob('*.jpg')) + list(folder.glob('*.png')))\n",
    "        print(f\"  {folder.name}: {img_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-architecture"
   },
   "source": [
    "## üèóÔ∏è Lightweight Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unet-generator"
   },
   "outputs": [],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2 if down else 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2) if down else nn.ReLU()\n",
    "        )\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5) if use_dropout else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_dropout and self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3, features=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (Downsampling) - 5 blocks for speed\n",
    "        self.down1 = nn.Conv2d(in_channels, features, 4, 2, 1)  # 128 -> 64\n",
    "        self.down2 = UNetBlock(features, features*2, down=True)  # 64 -> 32\n",
    "        self.down3 = UNetBlock(features*2, features*4, down=True)  # 32 -> 16\n",
    "        self.down4 = UNetBlock(features*4, features*8, down=True)  # 16 -> 8\n",
    "        self.down5 = UNetBlock(features*8, features*8, down=True)  # 8 -> 4\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1),  # 4 -> 2\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = nn.ConvTranspose2d(features*8, features*8, 4, 2, 1)  # 2 -> 4\n",
    "        self.up2 = nn.ConvTranspose2d(features*8*2, features*8, 4, 2, 1)  # 4 -> 8\n",
    "        self.up3 = nn.ConvTranspose2d(features*8*2, features*4, 4, 2, 1)  # 8 -> 16\n",
    "        self.up4 = nn.ConvTranspose2d(features*4*2, features*2, 4, 2, 1)  # 16 -> 32\n",
    "        self.up5 = nn.ConvTranspose2d(features*2*2, features, 4, 2, 1)    # 32 -> 64\n",
    "        \n",
    "        self.final = nn.ConvTranspose2d(features*2, out_channels, 4, 2, 1)  # 64 -> 128\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Batch norm layers for decoder\n",
    "        self.bn_up1 = nn.BatchNorm2d(features*8)\n",
    "        self.bn_up2 = nn.BatchNorm2d(features*8)\n",
    "        self.bn_up3 = nn.BatchNorm2d(features*4)\n",
    "        self.bn_up4 = nn.BatchNorm2d(features*2)\n",
    "        self.bn_up5 = nn.BatchNorm2d(features)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(d5)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        u1 = self.relu(self.bn_up1(self.up1(bottleneck)))\n",
    "        u1 = self.dropout(u1)\n",
    "        u1 = torch.cat([u1, d5], dim=1)\n",
    "        \n",
    "        u2 = self.relu(self.bn_up2(self.up2(u1)))\n",
    "        u2 = self.dropout(u2)\n",
    "        u2 = torch.cat([u2, d4], dim=1)\n",
    "        \n",
    "        u3 = self.relu(self.bn_up3(self.up3(u2)))\n",
    "        u3 = self.dropout(u3)\n",
    "        u3 = torch.cat([u3, d3], dim=1)\n",
    "        \n",
    "        u4 = self.relu(self.bn_up4(self.up4(u3)))\n",
    "        u4 = torch.cat([u4, d2], dim=1)\n",
    "        \n",
    "        u5 = self.relu(self.bn_up5(self.up5(u4)))\n",
    "        u5 = torch.cat([u5, d1], dim=1)\n",
    "        \n",
    "        return self.tanh(self.final(u5))\n",
    "\n",
    "# Test generator\n",
    "gen = Generator().to(device)\n",
    "test_input = torch.randn(1, 1, 128, 128).to(device)\n",
    "test_output = gen(test_input)\n",
    "print(f\"Generator output shape: {test_output.shape}\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in gen.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "patchgan-discriminator"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4, features=32):  # 1 (grayscale) + 3 (RGB) = 4\n",
    "        super().__init__()\n",
    "        \n",
    "        # PatchGAN discriminator with 4 layers\n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: 128x128 -> 64x64\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 2: 64x64 -> 32x32\n",
    "            nn.Conv2d(features, features*2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 3: 32x32 -> 16x16\n",
    "            nn.Conv2d(features*2, features*4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 4: 16x16 -> 8x8\n",
    "            nn.Conv2d(features*4, features*8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Final layer: 8x8 -> 1 (patch output)\n",
    "            nn.Conv2d(features*8, 1, 4, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, grayscale, rgb):\n",
    "        x = torch.cat([grayscale, rgb], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Test discriminator\n",
    "disc = Discriminator().to(device)\n",
    "test_gray = torch.randn(1, 1, 128, 128).to(device)\n",
    "test_rgb = torch.randn(1, 3, 128, 128).to(device)\n",
    "test_output = disc(test_gray, test_rgb)\n",
    "print(f\"Discriminator output shape: {test_output.shape}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in disc.parameters()):,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in gen.parameters()) + sum(p.numel() for p in disc.parameters())\n",
    "print(f\"\\nüéØ Total model parameters: {total_params:,} (Lightweight for fast training!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-preprocessing"
   },
   "source": [
    "## üìä Smart Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-class"
   },
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None, max_images=5000):\n",
    "        self.image_paths = image_paths[:max_images]  # Limit to 5K images\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"üì∏ Dataset initialized with {len(self.image_paths)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def rgb_to_lab(self, rgb_img):\n",
    "        \"\"\"Convert RGB to LAB color space\"\"\"\n",
    "        # Convert to numpy if tensor\n",
    "        if torch.is_tensor(rgb_img):\n",
    "            rgb_img = rgb_img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Ensure values are in [0, 1]\n",
    "        if rgb_img.max() > 1.0:\n",
    "            rgb_img = rgb_img / 255.0\n",
    "            \n",
    "        # Convert to LAB\n",
    "        lab_img = cv2.cvtColor(rgb_img.astype(np.float32), cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Normalize L channel to [-1, 1] and AB channels to [-1, 1]\n",
    "        L = lab_img[:, :, 0] / 50.0 - 1.0  # L: [0, 100] -> [-1, 1]\n",
    "        AB = lab_img[:, :, 1:] / 128.0     # AB: [-128, 127] -> [-1, 1]\n",
    "        \n",
    "        return torch.FloatTensor(L).unsqueeze(0), torch.FloatTensor(AB).permute(2, 0, 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load and convert image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # Convert to LAB\n",
    "            L, AB = self.rgb_to_lab(image)\n",
    "            \n",
    "            return L, AB  # L (grayscale) as input, AB (color) as target\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a dummy tensor if image fails to load\n",
    "            return torch.zeros(1, 128, 128), torch.zeros(2, 128, 128)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Fast 128x128 resolution\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dataset class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataloaders"
   },
   "outputs": [],
   "source": [
    "# Find all image files\n",
    "def find_images(data_dir, extensions=['.jpg', '.jpeg', '.png', '.bmp']):\n",
    "    image_paths = []\n",
    "    for ext in extensions:\n",
    "        image_paths.extend(list(data_dir.rglob(f'*{ext}')))\n",
    "        image_paths.extend(list(data_dir.rglob(f'*{ext.upper()}')))\n",
    "    return image_paths\n",
    "\n",
    "# Get all image paths\n",
    "all_image_paths = find_images(data_dir)\n",
    "print(f\"üìÅ Found {len(all_image_paths)} total images\")\n",
    "\n",
    "# Randomly sample 5000 images for speed\n",
    "random.shuffle(all_image_paths)\n",
    "selected_images = all_image_paths[:5000]\n",
    "print(f\"üéØ Selected {len(selected_images)} images for training\")\n",
    "\n",
    "# Train/Val split (80/20)\n",
    "split_idx = int(0.8 * len(selected_images))\n",
    "train_paths = selected_images[:split_idx]\n",
    "val_paths = selected_images[split_idx:]\n",
    "\n",
    "print(f\"üöÇ Training images: {len(train_paths)}\")\n",
    "print(f\"üîç Validation images: {len(val_paths)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ColorizationDataset(train_paths, transform=transform)\n",
    "val_dataset = ColorizationDataset(val_paths, transform=transform)\n",
    "\n",
    "# Create data loaders with optimizations\n",
    "batch_size = 64  # Larger batch size for speed\n",
    "num_workers = 2  # Optimized for Colab\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,  # Speed optimization\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ö° DataLoaders ready!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nüß™ Testing data loading...\")\n",
    "sample_L, sample_AB = next(iter(train_loader))\n",
    "print(f\"L (grayscale) batch shape: {sample_L.shape}\")\n",
    "print(f\"AB (color) batch shape: {sample_AB.shape}\")\n",
    "print(f\"L range: [{sample_L.min():.3f}, {sample_L.max():.3f}]\")\n",
    "print(f\"AB range: [{sample_AB.min():.3f}, {sample_AB.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-setup"
   },
   "source": [
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Training Setup with Speed Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loss-functions"
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def gan_loss(output, is_real):\n",
    "    \"\"\"GAN loss function\"\"\"\n",
    "    target = torch.ones_like(output) if is_real else torch.zeros_like(output)\n",
    "    return F.binary_cross_entropy(output, target)\n",
    "\n",
    "def l1_loss(output, target):\n",
    "    \"\"\"L1 loss for pixel-wise comparison\"\"\"\n",
    "    return F.l1_loss(output, target)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate schedulers\n",
    "g_scheduler = optim.lr_scheduler.ReduceLROnPlateau(g_optimizer, 'min', patience=5, factor=0.5)\n",
    "d_scheduler = optim.lr_scheduler.ReduceLROnPlateau(d_optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Loss weight\n",
    "lambda_l1 = 100  # Weight for L1 loss\n",
    "\n",
    "print(\"‚úÖ Training setup complete!\")\n",
    "print(f\"Generator LR: {g_optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"Discriminator LR: {d_optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"L1 Loss weight: {lambda_l1}\")\n",
    "print(f\"Mixed precision: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-utils"
   },
   "source": [
    "## üìä Visualization and Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization-functions"
   },
   "outputs": [],
   "source": [
    "def lab_to_rgb(L, AB):\n",
    "    \"\"\"Convert LAB to RGB for visualization\"\"\"\n",
    "    # Denormalize\n",
    "    L = (L + 1.0) * 50.0  # [-1, 1] -> [0, 100]\n",
    "    AB = AB * 128.0       # [-1, 1] -> [-128, 127]\n",
    "    \n",
    "    # Combine L and AB\n",
    "    Lab = torch.cat([L, AB], dim=1)\n",
    "    \n",
    "    # Convert to numpy and process each image in batch\n",
    "    rgb_images = []\n",
    "    for i in range(Lab.shape[0]):\n",
    "        lab_img = Lab[i].permute(1, 2, 0).cpu().numpy()\n",
    "        rgb_img = cv2.cvtColor(lab_img.astype(np.float32), cv2.COLOR_LAB2RGB)\n",
    "        rgb_img = np.clip(rgb_img, 0, 1)\n",
    "        rgb_images.append(torch.FloatTensor(rgb_img).permute(2, 0, 1))\n",
    "    \n",
    "    return torch.stack(rgb_images)\n",
    "\n",
    "def show_results(generator, val_loader, device, num_samples=8, epoch=0):\n",
    "    \"\"\"Display colorization results\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get a batch of validation data\n",
    "        L_batch, AB_batch = next(iter(val_loader))\n",
    "        L_batch = L_batch[:num_samples].to(device)\n",
    "        AB_batch = AB_batch[:num_samples].to(device)\n",
    "        \n",
    "        # Generate colorized images\n",
    "        with autocast():\n",
    "            fake_AB = generator(L_batch)\n",
    "        \n",
    "        # Convert to RGB for visualization\n",
    "        real_rgb = lab_to_rgb(L_batch, AB_batch)\n",
    "        fake_rgb = lab_to_rgb(L_batch, fake_AB)\n",
    "        grayscale = L_batch.repeat(1, 3, 1, 1)  # Convert to 3-channel grayscale\n",
    "        \n",
    "        # Create comparison grid\n",
    "        comparison = torch.cat([\n",
    "            grayscale.cpu(),\n",
    "            fake_rgb.cpu(),\n",
    "            real_rgb.cpu()\n",
    "        ], dim=0)\n",
    "        \n",
    "        grid = make_grid(comparison, nrow=num_samples, normalize=True, padding=2)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.imshow(grid.permute(1, 2, 0))\n",
    "        plt.title(f'Epoch {epoch} - Top: Grayscale, Middle: Generated, Bottom: Ground Truth')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    generator.train()\n",
    "\n",
    "def calculate_metrics(generator, val_loader, device, num_samples=100):\n",
    "    \"\"\"Calculate PSNR and SSIM metrics\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for L_batch, AB_batch in val_loader:\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            L_batch = L_batch.to(device)\n",
    "            AB_batch = AB_batch.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                fake_AB = generator(L_batch)\n",
    "            \n",
    "            # Convert to RGB\n",
    "            real_rgb = lab_to_rgb(L_batch, AB_batch)\n",
    "            fake_rgb = lab_to_rgb(L_batch, fake_AB)\n",
    "            \n",
    "            # Calculate metrics for each image in batch\n",
    "            for i in range(real_rgb.shape[0]):\n",
    "                if count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                real_img = real_rgb[i].permute(1, 2, 0).cpu().numpy()\n",
    "                fake_img = fake_rgb[i].permute(1, 2, 0).cpu().numpy()\n",
    "                \n",
    "                # PSNR\n",
    "                psnr_score = psnr(real_img, fake_img, data_range=1.0)\n",
    "                psnr_scores.append(psnr_score)\n",
    "                \n",
    "                # SSIM\n",
    "                ssim_score = ssim(real_img, fake_img, data_range=1.0, channel_axis=2)\n",
    "                ssim_scores.append(ssim_score)\n",
    "                \n",
    "                count += 1\n",
    "    \n",
    "    generator.train()\n",
    "    \n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
    "\n",
    "print(\"‚úÖ Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-loop"
   },
   "source": [
    "## üöÄ Memory-Efficient Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main-training"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 35  # 30-40 epochs for fast training\n",
    "save_interval = 10  # Save checkpoints every 10 epochs\n",
    "display_interval = 5  # Show results every 5 epochs\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/results', exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'g_loss': [],\n",
    "    'd_loss': [],\n",
    "    'psnr': [],\n",
    "    'ssim': []\n",
    "}\n",
    "\n",
    "print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
    "print(f\"üìä Training on {len(train_loader)} batches per epoch\")\n",
    "print(f\"‚ö° Using mixed precision training\")\n",
    "print(f\"üíæ Saving checkpoints every {save_interval} epochs\")\n",
    "\n",
    "# Training loop\n",
    "start_time = torch.cuda.Event(enable_timing=True)\n",
    "end_time = torch.cuda.Event(enable_timing=True)\n",
    "start_time.record()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = torch.cuda.Event(enable_timing=True)\n",
    "    epoch_end = torch.cuda.Event(enable_timing=True)\n",
    "    epoch_start.record()\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss = 0.0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    \n",
    "    for batch_idx, (L_batch, AB_batch) in enumerate(pbar):\n",
    "        batch_size = L_batch.size(0)\n",
    "        L_batch = L_batch.to(device)\n",
    "        AB_batch = AB_batch.to(device)\n",
    "        \n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            # Real images\n",
    "            real_output = discriminator(L_batch, AB_batch)\n",
    "            d_real_loss = gan_loss(real_output, True)\n",
    "            \n",
    "            # Fake images\n",
    "            fake_AB = generator(L_batch)\n",
    "            fake_output = discriminator(L_batch, fake_AB.detach())\n",
    "            d_fake_loss = gan_loss(fake_output, False)\n",
    "            \n",
    "            d_loss = (d_real_loss + d_fake_loss) * 0.5\n",
    "        \n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(d_optimizer)\n",
    "        \n",
    "        # -----------------\n",
    "        # Train Generator\n",
    "        # -----------------\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            # GAN loss\n",
    "            fake_output = discriminator(L_batch, fake_AB)\n",
    "            g_gan_loss = gan_loss(fake_output, True)\n",
    "            \n",
    "            # L1 loss\n",
    "            g_l1_loss = l1_loss(fake_AB, AB_batch)\n",
    "            \n",
    "            # Total generator loss\n",
    "            g_loss = g_gan_loss + lambda_l1 * g_l1_loss\n",
    "        \n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.step(g_optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update progress\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'G_loss': f'{g_loss.item():.4f}',\n",
    "            'D_loss': f'{d_loss.item():.4f}',\n",
    "            'L1': f'{g_l1_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_g_loss = epoch_g_loss / len(train_loader)\n",
    "    avg_d_loss = epoch_d_loss / len(train_loader)\n",
    "    \n",
    "    # Update learning rate\n",
    "    g_scheduler.step(avg_g_loss)\n",
    "    d_scheduler.step(avg_d_loss)\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    epoch_time = epoch_start.elapsed_time(epoch_end) / 1000  # Convert to seconds\n",
    "    \n",
    "    # Estimate remaining time\n",
    "    remaining_epochs = num_epochs - epoch - 1\n",
    "    eta = remaining_epochs * epoch_time / 60  # Convert to minutes\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - G_loss: {avg_g_loss:.4f}, D_loss: {avg_d_loss:.4f} - Time: {epoch_time:.1f}s - ETA: {eta:.1f}min')\n",
    "    \n",
    "    # Store history\n",
    "    history['g_loss'].append(avg_g_loss)\n",
    "    history['d_loss'].append(avg_d_loss)\n",
    "    \n",
    "    # Show results every few epochs\n",
    "    if (epoch + 1) % display_interval == 0:\n",
    "        print(f\"\\nüìä Epoch {epoch+1} Results:\")\n",
    "        show_results(generator, val_loader, device, num_samples=8, epoch=epoch+1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        psnr_score, ssim_score = calculate_metrics(generator, val_loader, device, num_samples=50)\n",
    "        history['psnr'].append(psnr_score)\n",
    "        history['ssim'].append(ssim_score)\n",
    "        print(f\"PSNR: {psnr_score:.2f} dB, SSIM: {ssim_score:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator': generator.state_dict(),\n",
    "            'discriminator': discriminator.state_dict(),\n",
    "            'g_optimizer': g_optimizer.state_dict(),\n",
    "            'd_optimizer': d_optimizer.state_dict(),\n",
    "            'history': history\n",
    "        }\n",
    "        torch.save(checkpoint, f'/content/checkpoints/checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"üíæ Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "# Calculate total training time\n",
    "end_time.record()\n",
    "torch.cuda.synchronize()\n",
    "total_time = start_time.elapsed_time(end_time) / 1000 / 60  # Convert to minutes\n",
    "\n",
    "print(f\"\\nüéâ Training completed in {total_time:.1f} minutes!\")\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'generator': generator.state_dict(),\n",
    "    'discriminator': discriminator.state_dict(),\n",
    "    'history': history\n",
    "}, '/content/final_model.pth')\n",
    "\n",
    "print(\"üíæ Final model saved as 'final_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-analysis"
   },
   "source": [
    "## üìà Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-training-curves"
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['g_loss'], label='Generator Loss', color='blue')\n",
    "axes[0, 0].plot(history['d_loss'], label='Discriminator Loss', color='red')\n",
    "axes[0, 0].set_title('Training Losses')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Generator loss only\n",
    "axes[0, 1].plot(history['g_loss'], color='blue')\n",
    "axes[0, 1].set_title('Generator Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# PSNR\n",
    "if history['psnr']:\n",
    "    epochs_with_metrics = range(display_interval, len(history['psnr']) * display_interval + 1, display_interval)\n",
    "    axes[1, 0].plot(epochs_with_metrics, history['psnr'], color='green', marker='o')\n",
    "    axes[1, 0].set_title('PSNR Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('PSNR (dB)')\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "# SSIM\n",
    "if history['ssim']:\n",
    "    axes[1, 1].plot(epochs_with_metrics, history['ssim'], color='orange', marker='o')\n",
    "    axes[1, 1].set_title('SSIM Score')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('SSIM')\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nüìä Final Training Results:\")\n",
    "print(f\"Final Generator Loss: {history['g_loss'][-1]:.4f}\")\n",
    "print(f\"Final Discriminator Loss: {history['d_loss'][-1]:.4f}\")\n",
    "if history['psnr']:\n",
    "    print(f\"Best PSNR: {max(history['psnr']):.2f} dB\")\n",
    "    print(f\"Final PSNR: {history['psnr'][-1]:.2f} dB\")\n",
    "if history['ssim']:\n",
    "    print(f\"Best SSIM: {max(history['ssim']):.4f}\")\n",
    "    print(f\"Final SSIM: {history['ssim'][-1]:.4f}\")\n",
    "print(f\"Total Training Time: {total_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-results"
   },
   "outputs": [],
   "source": [
    "# Generate final comparison images\n",
    "print(\"üé® Generating final comparison images...\")\n",
    "\n",
    "generator.eval()\n",
    "num_final_samples = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get validation samples\n",
    "    L_batch, AB_batch = next(iter(val_loader))\n",
    "    L_batch = L_batch[:num_final_samples].to(device)\n",
    "    AB_batch = AB_batch[:num_final_samples].to(device)\n",
    "    \n",
    "    # Generate colorized images\n",
    "    with autocast():\n",
    "        fake_AB = generator(L_batch)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    real_rgb = lab_to_rgb(L_batch, AB_batch)\n",
    "    fake_rgb = lab_to_rgb(L_batch, fake_AB)\n",
    "    grayscale = (L_batch + 1.0) / 2.0  # Convert to [0, 1] grayscale\n",
    "    grayscale = grayscale.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    # Create a comprehensive comparison\n",
    "    fig, axes = plt.subplots(3, num_final_samples, figsize=(20, 6))\n",
    "    \n",
    "    for i in range(num_final_samples):\n",
    "        # Grayscale input\n",
    "        axes[0, i].imshow(grayscale[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0, i].set_title('Input (Grayscale)' if i == 0 else '')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Generated colorization\n",
    "        axes[1, i].imshow(fake_rgb[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[1, i].set_title('Generated' if i == 0 else '')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[2, i].imshow(real_rgb[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[2, i].set_title('Ground Truth' if i == 0 else '')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Final Results - Fast Image Colorization with Pix2Pix GAN', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/final_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Calculate final comprehensive metrics\n",
    "print(\"\\nüìä Calculating final metrics on validation set...\")\n",
    "final_psnr, final_ssim = calculate_metrics(generator, val_loader, device, num_samples=200)\n",
    "\n",
    "print(f\"\\nüéØ Final Model Performance:\")\n",
    "print(f\"PSNR: {final_psnr:.2f} dB\")\n",
    "print(f\"SSIM: {final_ssim:.4f}\")\n",
    "print(f\"Model Size: {sum(p.numel() for p in generator.parameters()):,} parameters\")\n",
    "print(f\"Training Time: {total_time:.1f} minutes\")\n",
    "print(f\"Training Dataset: {len(train_dataset):,} images\")\n",
    "print(f\"Image Resolution: 128x128 pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-section"
   },
   "source": [
    "## üîÆ Inference Function for New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-function"
   },
   "outputs": [],
   "source": [
    "def colorize_image(image_path, generator, device, output_path=None):\n",
    "    \"\"\"\n",
    "    Colorize a single grayscale image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        generator: Trained generator model\n",
    "        device: Device to run inference on\n",
    "        output_path: Optional path to save colorized image\n",
    "    \n",
    "    Returns:\n",
    "        Colorized image as PIL Image\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Transform for model input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Convert to LAB\n",
    "    dataset = ColorizationDataset([], transform=None)\n",
    "    L, _ = dataset.rgb_to_lab(image_tensor.squeeze(0))\n",
    "    L = L.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate colorization\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            fake_AB = generator(L)\n",
    "        \n",
    "        # Convert back to RGB\n",
    "        colorized_rgb = lab_to_rgb(L, fake_AB)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        colorized_np = colorized_rgb[0].permute(1, 2, 0).cpu().numpy()\n",
    "        colorized_np = np.clip(colorized_np, 0, 1)\n",
    "        colorized_pil = Image.fromarray((colorized_np * 255).astype(np.uint8))\n",
    "        \n",
    "        # Resize back to original size\n",
    "        colorized_pil = colorized_pil.resize(original_size, Image.LANCZOS)\n",
    "        \n",
    "        # Save if output path provided\n",
    "        if output_path:\n",
    "            colorized_pil.save(output_path)\n",
    "            print(f\"üíæ Colorized image saved to: {output_path}\")\n",
    "        \n",
    "        return colorized_pil\n",
    "\n",
    "def batch_colorize(input_folder, output_folder, generator, device):\n",
    "    \"\"\"\n",
    "    Colorize all images in a folder\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing input images\n",
    "        output_folder: Path to folder to save colorized images\n",
    "        generator: Trained generator model\n",
    "        device: Device to run inference on\n",
    "    \"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(list(input_path.glob(f'*{ext}')))\n",
    "        image_files.extend(list(input_path.glob(f'*{ext.upper()}')))\n",
    "    \n",
    "    print(f\"üé® Found {len(image_files)} images to colorize\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_file in tqdm(image_files, desc=\"Colorizing images\"):\n",
    "        output_file = output_path / f\"colorized_{img_file.name}\"\n",
    "        try:\n",
    "            colorize_image(img_file, generator, device, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {img_file}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch colorization complete! Results saved to: {output_folder}\")\n",
    "\n",
    "# Example usage function\n",
    "def demo_inference():\n",
    "    \"\"\"\n",
    "    Demo function showing how to use the inference functions\n",
    "    \"\"\"\n",
    "    print(\"üîÆ Inference Functions Ready!\")\n",
    "    print(\"\\nüìù Usage Examples:\")\n",
    "    print(\"\\n1. Colorize a single image:\")\n",
    "    print(\"   colorized = colorize_image('path/to/image.jpg', generator, device, 'output.jpg')\")\n",
    "    print(\"\\n2. Batch colorize a folder:\")\n",
    "    print(\"   batch_colorize('input_folder/', 'output_folder/', generator, device)\")\n",
    "    print(\"\\n3. Load saved model for inference:\")\n",
    "    print(\"   checkpoint = torch.load('final_model.pth')\")\n",
    "    print(\"   generator.load_state_dict(checkpoint['generator'])\")\n",
    "    print(\"\\nüí° The model works best on images similar to the training data\")\n",
    "    print(\"   (natural scenes, people, objects)\")\n",
    "\n",
    "demo_inference()\n",
    "\n",
    "# Test inference on a validation image\n",
    "print(\"\\nüß™ Testing inference on a validation image...\")\n",
    "try:\n",
    "    # Use a validation image for demonstration\n",
    "    if val_paths:\n",
    "        test_image_path = val_paths[0]\n",
    "        colorized = colorize_image(test_image_path, generator, device, '/content/test_colorized.jpg')\n",
    "        \n",
    "        # Display comparison\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original image\n",
    "        original = Image.open(test_image_path)\n",
    "        axes[0].imshow(original)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Grayscale version\n",
    "        grayscale = original.convert('L')\n",
    "        axes[1].imshow(grayscale, cmap='gray')\n",
    "        axes[1].set_title('Grayscale Input')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Colorized\n",
    "        axes[2].imshow(colorized)\n",
    "        axes[2].set_title('AI Colorized')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/content/inference_demo.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Inference test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Inference test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-results"
   },
   "source": [
    "## üì• Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "# Create a zip file with all results\n",
    "import zipfile\n",
    "\n",
    "print(\"üì¶ Creating results package...\")\n",
    "\n",
    "with zipfile.ZipFile('/content/fast_colorization_results.zip', 'w') as zipf:\n",
    "    # Add model file\n",
    "    if os.path.exists('/content/final_model.pth'):\n",
    "        zipf.write('/content/final_model.pth', 'final_model.pth')\n",
    "    \n",
    "    # Add result images\n",
    "    if os.path.exists('/content/final_results.png'):\n",
    "        zipf.write('/content/final_results.png', 'final_results.png')\n",
    "    \n",
    "    # Add training curves\n",
    "    if os.path.exists('/content/training_curves.png'):\n",
    "        zipf.write('/content/training_curves.png', 'training_curves.png')\n",
    "    \n",
    "    # Add inference demo\n",
    "    if os.path.exists('/content/inference_demo.png'):\n",
    "        zipf.write('/content/inference_demo.png', 'inference_demo.png')\n",
    "    \n",
    "    # Add test colorized image\n",
    "    if os.path.exists('/content/test_colorized.jpg'):\n",
    "        zipf.write('/content/test_colorized.jpg', 'test_colorized.jpg')\n",
    "    \n",
    "    # Add latest checkpoint\n",
    "    checkpoint_files = [f for f in os.listdir('/content/checkpoints/') if f.endswith('.pth')]\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = sorted(checkpoint_files)[-1]\n",
    "        zipf.write(f'/content/checkpoints/{latest_checkpoint}', f'checkpoints/{latest_checkpoint}')\n",
    "\n",
    "print(\"‚úÖ Results package created: fast_colorization_results.zip\")\n",
    "\n",
    "# Download files (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Downloading results...\")\n",
    "    files.download('/content/fast_colorization_results.zip')\n",
    "    files.download('/content/final_model.pth')\n",
    "    print(\"‚úÖ Downloads complete!\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Not in Colab environment. Files saved locally.\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nüéâ Fast Image Colorization Training Complete!\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"‚Ä¢ Training Time: {total_time:.1f} minutes\")\n",
    "print(f\"‚Ä¢ Dataset Size: {len(train_dataset):,} training images\")\n",
    "print(f\"‚Ä¢ Model Size: {sum(p.numel() for p in generator.parameters()):,} parameters\")\n",
    "print(f\"‚Ä¢ Final PSNR: {final_psnr:.2f} dB\")\n",
    "print(f\"‚Ä¢ Final SSIM: {final_ssim:.4f}\")\n",
    "print(f\"‚Ä¢ Resolution: 128x128 pixels\")\n",
    "print(f\"\\nüöÄ Ready for inference on new images!\")\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "print(f\"‚Ä¢ final_model.pth - Trained model weights\")\n",
    "print(f\"‚Ä¢ final_results.png - Sample colorization results\")\n",
    "print(f\"‚Ä¢ training_curves.png - Training progress plots\")\n",
    "print(f\"‚Ä¢ fast_colorization_results.zip - Complete results package\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
